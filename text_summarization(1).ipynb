{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exstractive Text Summarization"
      ],
      "metadata": {
        "id": "6KQHMluhZSZR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txSTI1L-XrLZ",
        "outputId": "a297c69e-0e5d-4522-f481-731a05202182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fsZEC4zegsq",
        "outputId": "a08ab2e0-81a3-462e-b60e-0d7925ee42a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase and remove newline characters\n",
        "    text = text.lower().replace('\\n', ' ')\n",
        "\n",
        "    # Tokenize into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Tokenize each sentence into words, remove non-alphanumeric words, and filter out stop words\n",
        "    processed_sentences = []\n",
        "    tokenized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        words = [word for word in words if word.isalnum()]\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "        processed_sentences.append(' '.join(words))  # Join words back into a sentence\n",
        "        tokenized_sentences.append(words)  # Keep tokenized version as well\n",
        "\n",
        "    return processed_sentences, tokenized_sentences"
      ],
      "metadata": {
        "id": "VxLqAdk5enFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def calculate_tfidf_vectors(sentences):\n",
        "    # Join tokenized sentences back into strings for TF-IDF processing\n",
        "    joined_sentences = [' '.join(sentence) for sentence in sentences]\n",
        "\n",
        "    if len(joined_sentences) == 0:\n",
        "        raise ValueError(\"The input text is empty after preprocessing.\")\n",
        "\n",
        "    # Calculate TF-IDF matrix without removing stop words\n",
        "    vectorizer = TfidfVectorizer(stop_words=None)\n",
        "    tfidf_matrix = vectorizer.fit_transform(joined_sentences)\n",
        "\n",
        "    if tfidf_matrix.shape[1] == 0:\n",
        "        raise ValueError(\"TF-IDF failed: empty vocabulary, possibly due to stop words.\")\n",
        "\n",
        "    return tfidf_matrix.toarray()\n"
      ],
      "metadata": {
        "id": "xGLG4Gboe2Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentence_scores(tfidf_matrix):\n",
        "    # Compute the average TF-IDF vector for the document (overall importance)\n",
        "    document_vector = np.mean(tfidf_matrix, axis=0)\n",
        "    # Calculate cosine similarity of each sentence vector with the document vector\n",
        "    scores = cosine_similarity(tfidf_matrix, document_vector.reshape(1, -1)).flatten()\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "sukWgWsEfmRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_top_sentences(sentences, scores, top_k=3):\n",
        "    # Get indices of top-k sentences with highest similarity scores\n",
        "    top_sentence_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "    # Select the sentences by index\n",
        "    top_sentences = [sentences[i] for i in top_sentence_indices]\n",
        "    return ' '.join(top_sentences)\n"
      ],
      "metadata": {
        "id": "L9ftlPcPfojg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text, top_k=1):\n",
        "    # Step 1: Preprocess text\n",
        "    sentences, tokenized_sentences = preprocess_text(text)\n",
        "\n",
        "    # Step 2: Calculate TF-IDF vectors for each sentence\n",
        "    tfidf_matrix = calculate_tfidf_vectors(tokenized_sentences)\n",
        "\n",
        "    # Step 3: Calculate similarity scores for each sentence\n",
        "    scores = calculate_sentence_scores(tfidf_matrix)\n",
        "\n",
        "    # Step 4: Select top sentences as the summary\n",
        "    summary = select_top_sentences(sentences, scores, top_k)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "BQDRY5jVfvv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"Machine learning (ML) has become a cornerstone of modern technology, revolutionizing industries and reshaping the way we interact with the world. As a subset of artificial intelligence (AI), ML enables systems to learn and improve from experience without being explicitly programmed. Its importance spans various domains, from healthcare to finance, and from retail to transportation. In this article, we'll explore why ML is important, highlighting its transformative potential and the numerous benefits it offers.\"\n",
        "summary = summarize_text(text, top_k=1)\n",
        "print(\"Summary:\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "_DPLKBl2f0Of",
        "outputId": "3bd6b98d-7665-4b33-d979-d80103d2826f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0c88e1ade426>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"Machine learning (ML) has become a cornerstone of modern technology, revolutionizing industries and reshaping the way we interact with the world. As a subset of artificial intelligence (AI), ML enables systems to learn and improve from experience without being explicitly programmed. Its importance spans various domains, from healthcare to finance, and from retail to transportation. In this article, we'll explore why ML is important, highlighting its transformative potential and the numerous benefits it offers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Summary:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ba10e4137329>\u001b[0m in \u001b[0;36msummarize_text\u001b[0;34m(text, top_k)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Step 3: Calculate similarity scores for each sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_sentence_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Step 4: Select top sentences as the summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-673e77798c87>\u001b[0m in \u001b[0;36mcalculate_sentence_scores\u001b[0;34m(tfidf_matrix)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_sentence_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Compute the average TF-IDF vector for the document (overall importance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdocument_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Calculate cosine similarity of each sentence vector with the document vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tSgh3Pscf2Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "4NVdu0oRh8Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fvFAwqy0h-96"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}